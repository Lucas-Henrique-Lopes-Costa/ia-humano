# ğŸ“° Webscraper de NotÃ­cias

Um webscraper em Python que extrai manchetes e URLs de sites de notÃ­cias brasileiros populares.

## ğŸš€ CaracterÃ­sticas

- **MÃºltiplos Sites**: Extrai notÃ­cias do G1, UOL e Folha de S.Paulo
- **Tratamento de Erros**: Robusto contra falhas de rede e mudanÃ§as no layout dos sites
- **Respeitoso**: Inclui delays entre requisiÃ§Ãµes para nÃ£o sobrecarregar os servidores
- **FlexÃ­vel**: FÃ¡cil de estender para incluir novos sites de notÃ­cias
- **SaÃ­da Organizada**: Exibe as notÃ­cias no terminal e salva em arquivo

## ğŸ“‹ DependÃªncias

- `beautifulsoup4`: Para parsing do HTML
- `requests`: Para fazer requisiÃ§Ãµes HTTP
- `lxml`: Parser rÃ¡pido para BeautifulSoup

## ğŸ› ï¸ InstalaÃ§Ã£o

1. Clone este repositÃ³rio ou baixe os arquivos
2. Crie um ambiente virtual (recomendado):

   ```bash
   python -m venv .venv
   source .venv/bin/activate  # No Windows: .venv\Scripts\activate
   ```

3. Instale as dependÃªncias:

   ```bash
   pip install beautifulsoup4 requests lxml
   ```

## ğŸ“– Como Usar

### Uso BÃ¡sico

Execute o script principal:

```bash
python webscraper.py
```

O programa irÃ¡:

1. Extrair notÃ­cias dos sites configurados
2. Exibir as manchetes e URLs no terminal
3. Salvar as notÃ­cias em um arquivo `noticias.txt`

### Uso ProgramÃ¡tico

VocÃª tambÃ©m pode usar a classe `NewsScraper` em seus prÃ³prios scripts:

```python
from webscraper import NewsScraper

# Criar o scraper
scraper = NewsScraper(delay=1.0)

# Extrair notÃ­cias de todos os sites
noticias = scraper.get_all_news()

# Exibir as notÃ­cias
scraper.display_news(noticias)

# Ou extrair de um site especÃ­fico
noticias_g1 = scraper.extract_news_from_g1()
```

## ğŸ—ï¸ Estrutura do CÃ³digo

### Classe NewsScraper

- `__init__(delay)`: Inicializa o scraper com delay entre requisiÃ§Ãµes
- `fetch_page(url)`: Baixa uma pÃ¡gina e retorna objeto BeautifulSoup
- `extract_news_from_g1()`: Extrai notÃ­cias do G1
- `extract_news_from_uol()`: Extrai notÃ­cias do UOL
- `extract_news_from_folha()`: Extrai notÃ­cias da Folha de S.Paulo
- `get_all_news()`: Extrai notÃ­cias de todos os sites
- `display_news(news)`: Exibe notÃ­cias formatadas no terminal
- `save_to_file(news, filename)`: Salva notÃ­cias em arquivo

## ğŸ”§ PersonalizaÃ§Ã£o

### Adicionando Novos Sites

Para adicionar um novo site de notÃ­cias:

1. Crie um novo mÃ©todo `extract_news_from_seusite()`:

```python
def extract_news_from_seusite(self) -> List[Dict[str, str]]:
    url = "https://seusite.com/"
    soup = self.fetch_page(url)
    
    if not soup:
        return []
    
    news = []
    
    # Adapte os seletores CSS para o seu site
    links = soup.select('a.link-noticia')
    
    for link in links:
        title = link.get_text(strip=True)
        href = link.get('href')
        
        if title and href:
            full_url = urljoin(url, href)
            news.append({
                'titulo': title,
                'url': full_url,
                'fonte': 'Seu Site'
            })
    
    return news
```

2. Adicione o mÃ©todo Ã  lista de extractors em `get_all_news()`:

```python
extractors = [
    self.extract_news_from_g1,
    self.extract_news_from_uol,
    self.extract_news_from_folha,
    self.extract_news_from_seusite  # Adicione aqui
]
```

### Configurando Delays

Ajuste o delay entre requisiÃ§Ãµes para ser mais ou menos agressivo:

```python
# Mais rÃ¡pido (cuidado para nÃ£o ser bloqueado)
scraper = NewsScraper(delay=0.5)

# Mais respeitoso
scraper = NewsScraper(delay=2.0)
```

## âš ï¸ ConsideraÃ§Ãµes Importantes

### Aspectos Legais e Ã‰ticos

- **Respeite os robots.txt**: Sempre verifique o arquivo robots.txt dos sites
- **Use delays**: NÃ£o sobrecarregue os servidores com muitas requisiÃ§Ãµes
- **Termos de Uso**: Respeite os termos de uso dos sites
- **Uso Pessoal**: Este cÃ³digo Ã© para fins educacionais e uso pessoal

### LimitaÃ§Ãµes

- **MudanÃ§as no Layout**: Sites podem mudar seus layouts, quebrando os seletores
- **Rate Limiting**: Sites podem limitar o nÃºmero de requisiÃ§Ãµes
- **JavaScript**: Sites que dependem de JavaScript podem nÃ£o funcionar
- **Anti-Bot**: Alguns sites tÃªm proteÃ§Ãµes contra bots

## ğŸ›¡ï¸ Tratamento de Erros

O scraper inclui tratamento robusto de erros:

- **Timeouts**: RequisiÃ§Ãµes tÃªm timeout de 10 segundos
- **ExceÃ§Ãµes HTTP**: Trata erros de rede e status HTTP
- **Parsing**: Continua funcionando mesmo se um site falhar
- **Logging**: Registra todas as atividades e erros

## ğŸ“Š Exemplo de SaÃ­da

```
================================================================================
ğŸ“° NOTÃCIAS MAIS RECENTES (45 encontradas)
================================================================================

 1. [G1] Presidente anuncia novas medidas econÃ´micas para 2025
    ğŸ”— https://g1.globo.com/economia/noticia/2025/09/17/...

 2. [UOL] Copa do Mundo: Brasil confirma convocaÃ§Ã£o
    ğŸ”— https://www.uol.com.br/esporte/futebol/...

 3. [Folha de S.Paulo] Tecnologia brasileira ganha destaque internacional
    ğŸ”— https://www1.folha.uol.com.br/tec/...
```

## ğŸ¤ Contribuindo

Sinta-se Ã  vontade para:

- Reportar bugs
- Sugerir melhorias
- Adicionar novos sites
- Melhorar a documentaÃ§Ã£o

## ğŸ“ LicenÃ§a

Este projeto Ã© de cÃ³digo aberto para fins educacionais.

---

**Desenvolvido com â¤ï¸ para ajudar pessoas a se manterem informadas!**
