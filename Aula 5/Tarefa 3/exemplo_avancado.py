"""
Exemplo de Uso Avan√ßado do Webscraper
=====================================

Este arquivo demonstra como usar o webscraper de forma mais avan√ßada,
incluindo filtragem, an√°lise e relat√≥rios personalizados.
"""

from webscraper import NewsScraper
import json
from collections import Counter
import re


class AdvancedNewsAnalyzer(NewsScraper):
    """Classe estendida para an√°lise avan√ßada de not√≠cias."""

    def filter_news_by_keywords(self, news, keywords):
        """
        Filtra not√≠cias que contenham palavras-chave espec√≠ficas.

        Args:
            news: Lista de not√≠cias
            keywords: Lista de palavras-chave para filtrar

        Returns:
            Lista de not√≠cias filtradas
        """
        filtered_news = []

        for item in news:
            title_lower = item["titulo"].lower()
            if any(keyword.lower() in title_lower for keyword in keywords):
                filtered_news.append(item)

        return filtered_news

    def analyze_topics(self, news):
        """
        Analisa os t√≥picos mais comuns nas not√≠cias.

        Args:
            news: Lista de not√≠cias

        Returns:
            Dicion√°rio com estat√≠sticas dos t√≥picos
        """
        # Palavras comuns em portugu√™s que ser√£o ignoradas
        stop_words = {
            "de",
            "da",
            "do",
            "das",
            "dos",
            "e",
            "o",
            "a",
            "os",
            "as",
            "em",
            "na",
            "no",
            "nas",
            "nos",
            "para",
            "por",
            "com",
            "sem",
            "que",
            "se",
            "n√£o",
            "√©",
            "s√£o",
            "foi",
            "foram",
            "ser√°",
            "ser√£o",
            "tem",
            "t√™m",
            "teve",
            "tiveram",
            "ter√°",
            "ter√£o",
            "mais",
            "menos",
            "sobre",
            "ap√≥s",
            "antes",
            "durante",
            "contra",
            "entre",
            "at√©",
            "pela",
            "pelo",
            "pelas",
            "pelos",
            "sua",
            "seu",
            "suas",
            "seus",
        }

        all_words = []

        for item in news:
            # Remove pontua√ß√£o e converte para min√∫sculas
            title = re.sub(r"[^\w\s]", " ", item["titulo"].lower())
            words = title.split()

            # Filtra palavras significativas
            significant_words = [
                word for word in words if len(word) > 3 and word not in stop_words
            ]

            all_words.extend(significant_words)

        # Conta a frequ√™ncia das palavras
        word_counter = Counter(all_words)

        return {
            "total_words": len(all_words),
            "unique_words": len(word_counter),
            "most_common": word_counter.most_common(10),
            "news_count": len(news),
        }

    def generate_report(self, news):
        """
        Gera um relat√≥rio detalhado das not√≠cias.

        Args:
            news: Lista de not√≠cias

        Returns:
            Dicion√°rio com relat√≥rio completo
        """
        # An√°lise por fonte
        sources = Counter(item["fonte"] for item in news)

        # An√°lise de t√≥picos
        topics = self.analyze_topics(news)

        # An√°lise de URLs
        domains = Counter()
        for item in news:
            try:
                from urllib.parse import urlparse

                domain = urlparse(item["url"]).netloc
                domains[domain] += 1
            except:
                pass

        return {
            "summary": {
                "total_news": len(news),
                "sources": dict(sources),
                "domains": dict(domains.most_common(5)),
            },
            "topics": topics,
            "recent_news": news[:5],  # 5 mais recentes
        }

    def save_json_report(self, news, filename="relatorio_noticias.json"):
        """
        Salva um relat√≥rio em formato JSON.

        Args:
            news: Lista de not√≠cias
            filename: Nome do arquivo JSON
        """
        report = self.generate_report(news)

        try:
            with open(filename, "w", encoding="utf-8") as f:
                json.dump(report, f, ensure_ascii=False, indent=2)

            print(f"üìä Relat√≥rio JSON salvo em {filename}")

        except Exception as e:
            print(f"‚ùå Erro ao salvar relat√≥rio JSON: {e}")


def demo_basic_usage():
    """Demonstra√ß√£o de uso b√°sico."""
    print("=" * 60)
    print("üî• DEMONSTRA√á√ÉO - USO B√ÅSICO")
    print("=" * 60)

    scraper = NewsScraper(delay=1.0)

    # Extrair apenas do G1
    print("\nüì∞ Extraindo not√≠cias apenas do G1...")
    noticias_g1 = scraper.extract_news_from_g1()
    print(f"‚úÖ Encontradas {len(noticias_g1)} not√≠cias do G1")

    # Exibir apenas as 5 primeiras
    print("\nüîù Top 5 not√≠cias do G1:")
    for i, noticia in enumerate(noticias_g1[:5], 1):
        print(f"{i}. {noticia['titulo']}")


def demo_advanced_usage():
    """Demonstra√ß√£o de uso avan√ßado."""
    print("\n" + "=" * 60)
    print("üöÄ DEMONSTRA√á√ÉO - USO AVAN√áADO")
    print("=" * 60)

    analyzer = AdvancedNewsAnalyzer(delay=1.0)

    # Extrair todas as not√≠cias
    print("\nüìä Extraindo todas as not√≠cias...")
    all_news = analyzer.get_all_news()

    # Filtrar not√≠cias sobre pol√≠tica
    print("\nüèõÔ∏è Filtrando not√≠cias sobre pol√≠tica...")
    political_keywords = [
        "bolsonaro",
        "lula",
        "presidente",
        "c√¢mara",
        "senado",
        "pol√≠tica",
        "deputado",
    ]
    political_news = analyzer.filter_news_by_keywords(all_news, political_keywords)

    print(f"‚úÖ Encontradas {len(political_news)} not√≠cias sobre pol√≠tica:")
    for i, noticia in enumerate(political_news[:3], 1):
        print(f"  {i}. [{noticia['fonte']}] {noticia['titulo']}")

    # Filtrar not√≠cias sobre economia
    print("\nüí∞ Filtrando not√≠cias sobre economia...")
    economic_keywords = [
        "economia",
        "d√≥lar",
        "juros",
        "ibovespa",
        "mercado",
        "infla√ß√£o",
        "banco",
    ]
    economic_news = analyzer.filter_news_by_keywords(all_news, economic_keywords)

    print(f"‚úÖ Encontradas {len(economic_news)} not√≠cias sobre economia:")
    for i, noticia in enumerate(economic_news[:3], 1):
        print(f"  {i}. [{noticia['fonte']}] {noticia['titulo']}")

    # An√°lise de t√≥picos
    print("\nüîç Analisando t√≥picos mais comuns...")
    topics = analyzer.analyze_topics(all_news)

    print(f"üìà Estat√≠sticas:")
    print(f"  ‚Ä¢ Total de palavras: {topics['total_words']}")
    print(f"  ‚Ä¢ Palavras √∫nicas: {topics['unique_words']}")
    print(f"  ‚Ä¢ Total de not√≠cias: {topics['news_count']}")

    print(f"\nüèÜ Top 5 palavras mais comuns:")
    for word, count in topics["most_common"][:5]:
        print(f"  ‚Ä¢ {word}: {count} vezes")

    # Gerar relat√≥rio completo
    print("\nüìã Gerando relat√≥rio completo...")
    analyzer.save_json_report(all_news)

    # Salvar not√≠cias filtradas
    analyzer.save_to_file(political_news, "noticias_politica.txt")
    analyzer.save_to_file(economic_news, "noticias_economia.txt")


def demo_custom_scraping():
    """Demonstra√ß√£o de scraping personalizado."""
    print("\n" + "=" * 60)
    print("üõ†Ô∏è DEMONSTRA√á√ÉO - SCRAPING PERSONALIZADO")
    print("=" * 60)

    scraper = NewsScraper(delay=0.5)  # Mais r√°pido para demonstra√ß√£o

    # Exemplo de como adicionar um novo site
    def extract_news_from_custom_site(self, url, selectors):
        """M√©todo gen√©rico para extrair not√≠cias de qualquer site."""
        soup = self.fetch_page(url)

        if not soup:
            return []

        news = []

        for selector in selectors:
            links = soup.select(selector)
            for link in links:
                title = link.get_text(strip=True)
                href = link.get("href")

                if title and href and len(title) > 10:
                    from urllib.parse import urljoin

                    full_url = (
                        urljoin(url, href) if not href.startswith("http") else href
                    )

                    news.append(
                        {
                            "titulo": title,
                            "url": full_url,
                            "fonte": "Site Personalizado",
                        }
                    )

        return news[:10]  # Limite de 10 not√≠cias

    print("‚úÖ Fun√ß√£o de scraping personalizado criada!")
    print("üí° Esta fun√ß√£o pode ser usada para qualquer site de not√≠cias")
    print("   Basta fornecer a URL e os seletores CSS apropriados.")


if __name__ == "__main__":
    print("üéØ DEMONSTRA√á√ïES DO WEBSCRAPER DE NOT√çCIAS")
    print("=" * 60)

    try:
        # Executar demonstra√ß√µes
        demo_basic_usage()
        demo_advanced_usage()
        demo_custom_scraping()

        print("\n" + "=" * 60)
        print("üéâ TODAS AS DEMONSTRA√á√ïES CONCLU√çDAS!")
        print("=" * 60)
        print("\nüìÅ Arquivos gerados:")
        print("  ‚Ä¢ webscraper.py - C√≥digo principal")
        print("  ‚Ä¢ exemplo_avancado.py - Este arquivo")
        print("  ‚Ä¢ noticias.txt - Todas as not√≠cias")
        print("  ‚Ä¢ noticias_politica.txt - Not√≠cias de pol√≠tica")
        print("  ‚Ä¢ noticias_economia.txt - Not√≠cias de economia")
        print("  ‚Ä¢ relatorio_noticias.json - Relat√≥rio em JSON")

    except KeyboardInterrupt:
        print("\n\n‚ö†Ô∏è Demonstra√ß√£o interrompida pelo usu√°rio.")
    except Exception as e:
        print(f"\n\n‚ùå Erro durante a demonstra√ß√£o: {e}")
